// peggen.grmr
// Note that the extension is arbitrary

/*
root is the entry point for the parser. In most cases, it should be a repeated 
rule using the '+' (or even '*') operator or name another production which is. 
If you can write a grammar, which isn't, then you might have a predefined 
structure and there is probably a better way to parse or unmarshall.

There are three types of entries in the grammar.
1) a configuration entry
2) a special production entry
3) a production entry

root itself is a required production, but it doesn't have the same syntactic 
restrictions as a special_production
*/
root:
    (config \ special_production \ production)+

/*
configuration entries consist of ways to configure certain parameters in the 
parser generator. For now there are only 2:
1) import - This adds to a list an external source module to be imported into 
the "production implementation file". Multiple imports can exist in the grammar
 to specify multiple source modules.
2) export - This configures the base name of the exported module. Any 
subsequent appearances of this configuration entry will overwrite previous ones
If not provided, the name of the grammar file (extensions removed) is used.
*/
config:
    ('import' | 'export'), ':', nonws_printable

/*
normal productions of the grammar. They must be identifiers that are followed
by an optional comma-separated list of transform_functions. 
transform_functions identify functions that are applied to the nodes at
different stages of processing. All transform_functions must have the signature:

function_call(parser: Parser, node: Node) -> Node

Here the Parser itself acts as the context that can be used to manipulate the 
nodes. Within the function_call, you may compare against any of the productions
in the grammar to compare types by simply referring to the production 
identifier in all uppercase letters.

As a silly, contrived example, if you have productions "float" and "int" to 
distinguish the type of an "arithmetic" production, you might have in the 
grammar:
import: handlers
arithmetic(handle_arithmetic):
    float \ int
int:
    // definition of int
float:
    // definition of float

# in handlers.py
class FloatNode(Node):
    __slots__ = ("val",)
    def __init__(self, source_node, float_val):
        # copy data from source_node
        self.val = float_val
class IntNode(Node):
    __slots__ = ("val",)
    def __init__(self, source_node, integer_val):
        # copy data from source_node
        self.val = integer_val
def handle_arithmetic(parser, arithmetic_node)
    if arithmetic_node[0].rule is INT:
        return IntNode(arithmetic_node, int(str(parser.get_tokens(arithmetic_node)[0])))
    elif arithmetic_node[0].rule is FLOAT:
        return FloatNode(arithmetic_node, float(str(parser.get_tokens(arithmetic_node)[0])))
    return FAIL_NODE

Options are currently (ordered by index in list)
1) The first one is applied while building the AST. The result is the Node 
stored in the cache for packrat so it on any given string and at any location, 
this function is only called once.
2) The second one (not yet implemented) is applied during traversal after AST is built. Each of the 
grammar operators has a default for inorder traversal and is initiated by 
parser_instance.traverse()
*/
production:
    identifier, ('(', transform_functions, ')')?, ':', long_choice
transform_functions:
    ','.nonws_printable

/*
special_productions are those that have some special meaning or processing for
the ParserGenerator to work correctly. 
token is required in each grammar for tokenization, but punctuator and keyword are not
(though they will often appear, especially punctuator)
Note that they may NOT be transformed directly.
*/
special_production:
    ('token' | 'root'), ':', base_rule |
    ('punctuator' | 'keyword'), ':', ','.string_literal

/*
token is the entry point for the tokenizer. This generally comprises the 
"special_production"s, whitespace, and some form of identifiers. This is the 
production that you should definitely focus on using the `\` operator. It 
is pretty easy to mess up the tokenizer step by using the wrong one.

The resulting node of the token production should NEVER result in more than one
"token". This would otherwise likely result in a broken parser. The guideline 
is basically that if you operators other than '\' or '|' are present when the 
rule is fully expanded, it is probably wrong. If you find the need to know more
information of nearby tokens (implying your grammar is not really context 
free), I suggest making a separate production that uses lookaheads/lookbehinds 
or adding some context analysis to a subclass of the parser. The latter can be
implemented using the transform functions
*/
token:
    whitespace \ string_literal \ regex_literal \ punctuator \ keyword \ identifier

/*
punctuators are general characters that delimit other non-whitespace tokens in 
grammar. The implementation is such that each punctuator has its own production
made and then an overall production representing any (generally by '|' linking 
each one)
*/
punctuator:
    '!','\\', ',', '?', '.', '&', ':', '+', '*', '(', ')', '{', '}' // to include '/' in the future for a proper "first_choice"
/*
same as punctuator but separated for ease of semantics to identify keywords
*/
keyword:
    'import', 'export', 'punctuator', 'keyword', 'token', 'root'

/*
productions long_choice through base_rule implement the grammar operators and 
their precedence as described in How peggen works
*/
long_choice(simplify_rule):
    '|'.first_choice
first_choice(simplify_rule):
    '\'.sequence     // eventually will be ('\' | '/').sequence
sequence(simplify_rule):
    ','.repeated_rule
repeated_rule(simplify_rule):
    ('+' | '*' | '?' | '{', \d*, ':', \d* '}')?, list_rule
list_rule(simplify_rule):
    '.'.lookahead_rule
lookahead_rule(simplify_rule):
    ('&' | '!')?, base_rule
base_rule:
    terminal | nonterminal | identifier | '(', long_choice, ')'

/*
terminal string_literals or regex_literals
*/
terminal:
    string_literal \ regex_literal
string_literal:
    "'(((?<=\\\\)\')|[^\'])*'"
regex_literal:
    "\"(((?<=\\\\)\")|[^\"])*\""

/*
nonterminals identify other productions
*/
nonterminal:
    identifier
identifier:
    "\w+"

/*
For specific options that need not be restricted to alphanumeric + '_' "words"
*/
nonws_printable:
    "\S+"

/*
whitespace. skip_token is a built-in transform function that takes a successful
node built during tokenization and marks it for skipping so that it is not used
during parsing. If whitespace is significant or needed for parsing
*/
whitespace(skip_token):
    "(\s+|//[^\n]*\n|/\*.*\*/)+" // includes C-style comments as whitespace